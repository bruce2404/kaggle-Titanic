#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ³°å¦å°¼å…‹å·ç”Ÿå­˜é¢„æµ‹ - å¹³è¡¡ä¼˜åŒ–ç‰ˆæœ¬
åŸºäºKaggleå®é™…åˆ†æ•°åé¦ˆçš„æ”¹è¿›ç‰ˆæœ¬
é‡ç‚¹è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹³è¡¡æ¨¡å‹å¤æ‚åº¦ä¸æ³›åŒ–èƒ½åŠ›
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class TitanicSurvivalPredictorBalanced:
    def __init__(self):
        self.train_data = None
        self.test_data = None
        self.models = {}
        self.scaler = StandardScaler()
        self.feature_columns = []
        self.best_model = None
        
    def load_data(self, train_path, test_path):
        """
        åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        """
        print("=== æ•°æ®åŠ è½½é˜¶æ®µ ===")
        self.train_data = pd.read_csv(train_path)
        self.test_data = pd.read_csv(test_path)
        
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {self.train_data.shape}")
        print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {self.test_data.shape}")
        print(f"è®­ç»ƒæ•°æ®ç¼ºå¤±å€¼:\n{self.train_data.isnull().sum()}")
        
    def exploratory_data_analysis(self):
        """
        æ¢ç´¢æ€§æ•°æ®åˆ†æ
        """
        print("\n=== æ¢ç´¢æ€§æ•°æ®åˆ†æé˜¶æ®µ ===")
        
        # ç”Ÿå­˜ç‡ç»Ÿè®¡
        survival_rate = self.train_data['Survived'].mean()
        print(f"æ€»ä½“ç”Ÿå­˜ç‡: {survival_rate:.2%}")
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('æ³°å¦å°¼å…‹å·æ•°æ®æ¢ç´¢åˆ†æ - å¹³è¡¡ç‰ˆæœ¬', fontsize=16)
        
        # æ€§åˆ«ä¸ç”Ÿå­˜ç‡
        sex_survival = self.train_data.groupby('Sex')['Survived'].mean()
        axes[0,0].bar(sex_survival.index, sex_survival.values)
        axes[0,0].set_title('æ€§åˆ«ä¸ç”Ÿå­˜ç‡')
        axes[0,0].set_ylabel('ç”Ÿå­˜ç‡')
        
        # èˆ¹èˆ±ç­‰çº§ä¸ç”Ÿå­˜ç‡
        class_survival = self.train_data.groupby('Pclass')['Survived'].mean()
        axes[0,1].bar(class_survival.index, class_survival.values)
        axes[0,1].set_title('èˆ¹èˆ±ç­‰çº§ä¸ç”Ÿå­˜ç‡')
        axes[0,1].set_ylabel('ç”Ÿå­˜ç‡')
        
        # å¹´é¾„åˆ†å¸ƒ
        axes[0,2].hist(self.train_data['Age'].dropna(), bins=30, alpha=0.7)
        axes[0,2].set_title('å¹´é¾„åˆ†å¸ƒ')
        axes[0,2].set_xlabel('å¹´é¾„')
        
        # ç¥¨ä»·åˆ†å¸ƒ
        axes[1,0].hist(self.train_data['Fare'].dropna(), bins=30, alpha=0.7)
        axes[1,0].set_title('ç¥¨ä»·åˆ†å¸ƒ')
        axes[1,0].set_xlabel('ç¥¨ä»·')
        
        # ç™»èˆ¹æ¸¯å£ä¸ç”Ÿå­˜ç‡
        embarked_survival = self.train_data.groupby('Embarked')['Survived'].mean()
        axes[1,1].bar(embarked_survival.index, embarked_survival.values)
        axes[1,1].set_title('ç™»èˆ¹æ¸¯å£ä¸ç”Ÿå­˜ç‡')
        axes[1,1].set_ylabel('ç”Ÿå­˜ç‡')
        
        # å®¶åº­å¤§å°ä¸ç”Ÿå­˜ç‡
        self.train_data['Family_Size'] = self.train_data['SibSp'] + self.train_data['Parch'] + 1
        family_survival = self.train_data.groupby('Family_Size')['Survived'].mean()
        axes[1,2].plot(family_survival.index, family_survival.values, marker='o')
        axes[1,2].set_title('å®¶åº­å¤§å°ä¸ç”Ÿå­˜ç‡')
        axes[1,2].set_xlabel('å®¶åº­å¤§å°')
        axes[1,2].set_ylabel('ç”Ÿå­˜ç‡')
        
        plt.tight_layout()
        plt.savefig('titanic_eda_balanced.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def balanced_feature_engineering(self):
        """
        å¹³è¡¡çš„ç‰¹å¾å·¥ç¨‹ - ç®€åŒ–ä½†æœ‰æ•ˆ
        """
        print("\n=== å¹³è¡¡ç‰¹å¾å·¥ç¨‹é˜¶æ®µ ===")
        
        # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        all_data = pd.concat([self.train_data, self.test_data], ignore_index=True)
        
        # 1. åŸºç¡€ç¼ºå¤±å€¼å¤„ç†ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        print("å¤„ç†ç¼ºå¤±å€¼...")
        
        # Age: ä½¿ç”¨ä¸­ä½æ•°æŒ‰æ€§åˆ«å’Œèˆ¹èˆ±ç­‰çº§åˆ†ç»„å¡«å……
        all_data['Age'] = all_data.groupby(['Sex', 'Pclass'])['Age'].transform(
            lambda x: x.fillna(x.median())
        )
        all_data['Age'].fillna(all_data['Age'].median(), inplace=True)
        
        # Embarked: ä½¿ç”¨ä¼—æ•°å¡«å……
        all_data['Embarked'].fillna(all_data['Embarked'].mode()[0], inplace=True)
        
        # Fare: ä½¿ç”¨ä¸­ä½æ•°æŒ‰èˆ¹èˆ±ç­‰çº§å¡«å……
        all_data['Fare'] = all_data.groupby('Pclass')['Fare'].transform(
            lambda x: x.fillna(x.median())
        )
        
        # 2. æ ¸å¿ƒç‰¹å¾å·¥ç¨‹ï¼ˆç»è¿‡éªŒè¯çš„ç¨³å®šç‰¹å¾ï¼‰
        print("åˆ›å»ºæ ¸å¿ƒç‰¹å¾...")
        
        # 2.1 èˆ¹èˆ±ç‰¹å¾
        all_data['Has_Cabin'] = all_data['Cabin'].notna().astype(int)
        
        # 2.2 å®¶åº­ç‰¹å¾
        all_data['Family_Size'] = all_data['SibSp'] + all_data['Parch'] + 1
        all_data['Is_Alone'] = (all_data['Family_Size'] == 1).astype(int)
        
        # 2.3 ç§°è°“ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        all_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
        
        # ç®€åŒ–çš„ç§°è°“åˆ†ç»„
        title_mapping = {
            'Mr': 'Mr',
            'Mrs': 'Mrs', 
            'Miss': 'Miss',
            'Master': 'Master'
        }
        all_data['Title'] = all_data['Title'].map(title_mapping)
        all_data['Title'].fillna('Other', inplace=True)
        
        # 2.4 å¹´é¾„åˆ†ç»„ï¼ˆç®€åŒ–ï¼‰
        all_data['Age_Group'] = pd.cut(all_data['Age'], 
                                      bins=[0, 16, 32, 48, 80], 
                                      labels=['Child', 'Young', 'Adult', 'Senior'])
        all_data['Age_Group'] = all_data['Age_Group'].astype(str)
        
        # 2.5 ç¥¨ä»·åˆ†ç»„ï¼ˆç®€åŒ–ï¼‰
        all_data['Fare_Group'] = pd.qcut(all_data['Fare'], 
                                        q=4, 
                                        labels=['Low', 'Medium', 'High', 'Very_High'])
        all_data['Fare_Group'] = all_data['Fare_Group'].astype(str)
        
        # 2.6 å®¶åº­ç±»å‹ï¼ˆç®€åŒ–ï¼‰
        all_data['Family_Type'] = 'Single'
        all_data.loc[all_data['Family_Size'].between(2, 4), 'Family_Type'] = 'Small'
        all_data.loc[all_data['Family_Size'] > 4, 'Family_Type'] = 'Large'
        
        # 3. ç‰¹å¾ç¼–ç 
        print("ç¼–ç ç‰¹å¾...")
        
        # æ€§åˆ«ç¼–ç 
        all_data['Sex'] = all_data['Sex'].map({'male': 0, 'female': 1})
        
        # ç™»èˆ¹æ¸¯å£ç¼–ç 
        embarked_mapping = {'S': 0, 'C': 1, 'Q': 2}
        all_data['Embarked'] = all_data['Embarked'].map(embarked_mapping)
        
        # ç§°è°“ç¼–ç 
        title_mapping = {'Mr': 0, 'Mrs': 1, 'Miss': 2, 'Master': 3, 'Other': 4}
        all_data['Title'] = all_data['Title'].map(title_mapping)
        
        # å¹´é¾„ç»„ç¼–ç 
        age_group_mapping = {'Child': 0, 'Young': 1, 'Adult': 2, 'Senior': 3}
        all_data['Age_Group'] = all_data['Age_Group'].map(age_group_mapping)
        
        # ç¥¨ä»·ç»„ç¼–ç 
        fare_group_mapping = {'Low': 0, 'Medium': 1, 'High': 2, 'Very_High': 3}
        all_data['Fare_Group'] = all_data['Fare_Group'].map(fare_group_mapping)
        
        # å®¶åº­ç±»å‹ç¼–ç 
        family_type_mapping = {'Single': 0, 'Small': 1, 'Large': 2}
        all_data['Family_Type'] = all_data['Family_Type'].map(family_type_mapping)
        
        # 4. é€‰æ‹©æœ€ç»ˆç‰¹å¾ï¼ˆä¿å®ˆé€‰æ‹©ï¼‰
        self.feature_columns = [
            # åŸºç¡€ç‰¹å¾
            'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked',
            # æ ¸å¿ƒè¡ç”Ÿç‰¹å¾
            'Has_Cabin', 'Family_Size', 'Is_Alone', 'Title',
            'Age_Group', 'Fare_Group', 'Family_Type'
        ]
        
        # åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        train_len = len(self.train_data)
        self.train_processed = all_data[:train_len][self.feature_columns + ['Survived']]
        self.test_processed = all_data[train_len:][self.feature_columns]
        
        print(f"å¹³è¡¡ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾æ•°é‡: {len(self.feature_columns)}")
        print(f"ç‰¹å¾åˆ—è¡¨: {self.feature_columns}")
        
    def train_balanced_models(self):
        """
        è®­ç»ƒå¹³è¡¡çš„æœºå™¨å­¦ä¹ æ¨¡å‹
        """
        print("\n=== å¹³è¡¡æ¨¡å‹è®­ç»ƒé˜¶æ®µ ===")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = self.train_processed.drop('Survived', axis=1)
        y = self.train_processed['Survived']
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # å®šä¹‰å¹³è¡¡çš„æ¨¡å‹ï¼ˆå‡å°‘å¤æ‚åº¦ï¼Œå¢å¼ºæ³›åŒ–ï¼‰
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=100, max_depth=6, min_samples_split=10,
                min_samples_leaf=5, random_state=42, n_jobs=-1
            ),
            'GradientBoosting': GradientBoostingClassifier(
                n_estimators=100, learning_rate=0.1, max_depth=4, 
                subsample=0.8, random_state=42
            ),
            'LogisticRegression': LogisticRegression(
                C=1.0, random_state=42, max_iter=1000
            ),
            'SVM': SVC(
                C=1.0, kernel='rbf', gamma='scale', probability=True, random_state=42
            )
        }
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
        model_scores = {}
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for name, model in models.items():
            print(f"\nè®­ç»ƒ {name} æ¨¡å‹...")
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
            model_scores[name] = cv_scores.mean()
            
            print(f"{name} äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train, y_train)
            
            # éªŒè¯é›†é¢„æµ‹
            val_pred = model.predict(X_val)
            val_accuracy = accuracy_score(y_val, val_pred)
            print(f"{name} éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f}")
            
            # ä¿å­˜æ¨¡å‹
            self.models[name] = model
        
        # åˆ›å»ºä¿å®ˆçš„é›†æˆæ¨¡å‹
        print("\nåˆ›å»ºå¹³è¡¡é›†æˆæ¨¡å‹...")
        voting_clf = VotingClassifier(
            estimators=[
                ('rf', models['RandomForest']),
                ('gb', models['GradientBoosting']),
                ('lr', models['LogisticRegression']),
                ('svm', models['SVM'])
            ],
            voting='soft'
        )
        
        # è®­ç»ƒé›†æˆæ¨¡å‹
        voting_clf.fit(X_train, y_train)
        self.models['Balanced_Ensemble'] = voting_clf
        
        # è¯„ä¼°é›†æˆæ¨¡å‹
        ensemble_cv_scores = cross_val_score(voting_clf, X_train, y_train, cv=cv, scoring='accuracy')
        model_scores['Balanced_Ensemble'] = ensemble_cv_scores.mean()
        
        ensemble_val_pred = voting_clf.predict(X_val)
        ensemble_val_accuracy = accuracy_score(y_val, ensemble_val_pred)
        
        print(f"\nBalanced_Ensemble äº¤å‰éªŒè¯å‡†ç¡®ç‡: {ensemble_cv_scores.mean():.4f} (+/- {ensemble_cv_scores.std() * 2:.4f})")
        print(f"Balanced_Ensemble éªŒè¯é›†å‡†ç¡®ç‡: {ensemble_val_accuracy:.4f}")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model_name = max(model_scores, key=model_scores.get)
        self.best_model = self.models[best_model_name]
        
        print(f"\næœ€ä½³æ¨¡å‹: {best_model_name} (CVåˆ†æ•°: {model_scores[best_model_name]:.4f})")
        
        return model_scores
        
    def make_predictions(self):
        """
        ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹
        """
        print("\n=== é¢„æµ‹é˜¶æ®µ ===")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        X_test = self.scaler.transform(self.test_processed)
        
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹é¢„æµ‹
        predictions = self.best_model.predict(X_test)
        
        # åˆ›å»ºæäº¤æ–‡ä»¶
        submission = pd.DataFrame({
            'PassengerId': self.test_data['PassengerId'],
            'Survived': predictions.astype(int)  # ç¡®ä¿æ˜¯æ•´æ•°æ ¼å¼
        })
        
        submission.to_csv('titanic_submission_balanced.csv', index=False)
        print(f"é¢„æµ‹å®Œæˆï¼Œç”Ÿå­˜äººæ•°: {predictions.sum()}/{len(predictions)}")
        
        return submission
        
    def feature_importance_analysis(self):
        """
        åˆ†æç‰¹å¾é‡è¦æ€§
        """
        print("\n=== ç‰¹å¾é‡è¦æ€§åˆ†æ ===")
        
        # è·å–éšæœºæ£®æ—æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        if 'RandomForest' in self.models:
            rf_model = self.models['RandomForest']
            feature_names = self.feature_columns
            
            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
            importance_df = pd.DataFrame({
                'ç‰¹å¾': feature_names,
                'é‡è¦æ€§': rf_model.feature_importances_
            }).sort_values('é‡è¦æ€§', ascending=False)
            
            print("\nç‰¹å¾é‡è¦æ€§æ’åº (éšæœºæ£®æ—):")
            for idx, row in importance_df.iterrows():
                print(f"{row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}")
            
            # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
            plt.figure(figsize=(10, 8))
            sns.barplot(data=importance_df, x='é‡è¦æ€§', y='ç‰¹å¾')
            plt.title('ç‰¹å¾é‡è¦æ€§åˆ†æ (å¹³è¡¡ç‰ˆæœ¬)')
            plt.xlabel('é‡è¦æ€§åˆ†æ•°')
            plt.tight_layout()
            plt.savefig('feature_importance_balanced.png', dpi=300, bbox_inches='tight')
            plt.show()
    
    def run_complete_balanced_pipeline(self, train_path, test_path):
        """
        è¿è¡Œå®Œæ•´çš„å¹³è¡¡æœºå™¨å­¦ä¹ æµæ°´çº¿
        """
        print("ğŸš¢ æ³°å¦å°¼å…‹å·ç”Ÿå­˜é¢„æµ‹ - å¹³è¡¡ç‰ˆæœ¬æµæ°´çº¿å¯åŠ¨ ğŸš¢")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data(train_path, test_path)
        
        # 2. æ¢ç´¢æ€§æ•°æ®åˆ†æ
        self.exploratory_data_analysis()
        
        # 3. å¹³è¡¡ç‰¹å¾å·¥ç¨‹
        self.balanced_feature_engineering()
        
        # 4. æ¨¡å‹è®­ç»ƒ
        model_scores = self.train_balanced_models()
        
        # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
        self.feature_importance_analysis()
        
        # 6. é¢„æµ‹
        submission = self.make_predictions()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ å¹³è¡¡ç‰ˆæœ¬æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½æ€»ç»“:")
        for model_name, score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True):
            print(f"   {model_name}: {score:.4f}")
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print("   - titanic_submission_balanced.csv (å¹³è¡¡é¢„æµ‹ç»“æœ)")
        print("   - titanic_eda_balanced.png (æ•°æ®åˆ†æå›¾è¡¨)")
        print("   - feature_importance_balanced.png (ç‰¹å¾é‡è¦æ€§å›¾è¡¨)")
        print("\nğŸ’¡ å¹³è¡¡ç‰ˆæœ¬ç‰¹ç‚¹:")
        print("   - ç®€åŒ–ç‰¹å¾å·¥ç¨‹ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©")
        print("   - ä¿å®ˆçš„æ¨¡å‹å‚æ•°ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›")
        print("   - ä¿ç•™ç»è¿‡éªŒè¯çš„æ ¸å¿ƒç‰¹å¾")
        print("   - å¹³è¡¡å¤æ‚åº¦ä¸ç¨³å®šæ€§")
        
        return submission

# ä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºå¹³è¡¡é¢„æµ‹å™¨å®ä¾‹
    predictor = TitanicSurvivalPredictorBalanced()
    
    # è¿è¡Œå®Œæ•´å¹³è¡¡æµæ°´çº¿
    submission = predictor.run_complete_balanced_pipeline('train.csv', 'test.csv')
    
    print("\nğŸ† å¹³è¡¡ç‰ˆæœ¬é¢„æµ‹å®Œæˆï¼æœŸå¾…æ›´ç¨³å®šçš„Kaggleåˆ†æ•°ï¼")