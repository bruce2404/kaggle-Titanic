# -*- coding: utf-8 -*-
"""
æ³°å¦å°¼å…‹å·ç”Ÿå­˜é¢„æµ‹æ¨¡å‹ - ä¼˜åŒ–ç‰ˆæœ¬
åŸºäºChris Deotteé«˜åˆ†æ–¹æ¡ˆçš„æ”¹è¿›å®ç°
ç›®æ ‡ï¼šé€šè¿‡æ·±åº¦ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹ä¼˜åŒ–æå‡é¢„æµ‹å‡†ç¡®ç‡
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

class TitanicSurvivalPredictorOptimized:
    """
    æ³°å¦å°¼å…‹å·ç”Ÿå­˜é¢„æµ‹å™¨ - ä¼˜åŒ–ç‰ˆæœ¬
    åŸºäºé«˜åˆ†æ–¹æ¡ˆçš„æ·±åº¦ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹ä¼˜åŒ–
    """
    
    def __init__(self):
        self.train_data = None
        self.test_data = None
        self.models = {}
        self.scaler = StandardScaler()
        self.label_encoders = {}
        
    def load_data(self, train_path, test_path):
        """
        åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        """
        print("=== æ•°æ®åŠ è½½é˜¶æ®µ ===")
        self.train_data = pd.read_csv(train_path)
        self.test_data = pd.read_csv(test_path)
        
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {self.train_data.shape}")
        print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {self.test_data.shape}")
        print("\nè®­ç»ƒæ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(self.train_data.info())
        
    def exploratory_data_analysis(self):
        """
        æ¢ç´¢æ€§æ•°æ®åˆ†æ
        """
        print("\n=== æ¢ç´¢æ€§æ•°æ®åˆ†æ ===")
        
        # ç”Ÿå­˜ç‡ç»Ÿè®¡
        survival_rate = self.train_data['Survived'].mean()
        print(f"æ€»ä½“ç”Ÿå­˜ç‡: {survival_rate:.2%}")
        
        # ç¼ºå¤±å€¼åˆ†æ
        print("\nè®­ç»ƒæ•°æ®ç¼ºå¤±å€¼ç»Ÿè®¡:")
        missing_train = self.train_data.isnull().sum()
        missing_train_pct = (missing_train / len(self.train_data)) * 100
        missing_df = pd.DataFrame({
            'ç¼ºå¤±æ•°é‡': missing_train,
            'ç¼ºå¤±æ¯”ä¾‹(%)': missing_train_pct
        })
        print(missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0])
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        self._create_visualizations()
        
    def _create_visualizations(self):
        """
        åˆ›å»ºæ•°æ®å¯è§†åŒ–å›¾è¡¨
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('æ³°å¦å°¼å…‹å·æ•°æ®æ¢ç´¢æ€§åˆ†æ - ä¼˜åŒ–ç‰ˆæœ¬', fontsize=16, fontweight='bold')
        
        # 1. ç”Ÿå­˜ç‡åˆ†å¸ƒ
        survival_counts = self.train_data['Survived'].value_counts()
        axes[0, 0].pie(survival_counts.values, labels=['æ­»äº¡', 'ç”Ÿå­˜'], autopct='%1.1f%%', startangle=90)
        axes[0, 0].set_title('ç”Ÿå­˜ç‡åˆ†å¸ƒ')
        
        # 2. æ€§åˆ«ä¸ç”Ÿå­˜ç‡
        sns.barplot(data=self.train_data, x='Sex', y='Survived', ax=axes[0, 1])
        axes[0, 1].set_title('æ€§åˆ«ä¸ç”Ÿå­˜ç‡')
        axes[0, 1].set_ylabel('ç”Ÿå­˜ç‡')
        
        # 3. èˆ¹èˆ±ç­‰çº§ä¸ç”Ÿå­˜ç‡
        sns.barplot(data=self.train_data, x='Pclass', y='Survived', ax=axes[0, 2])
        axes[0, 2].set_title('èˆ¹èˆ±ç­‰çº§ä¸ç”Ÿå­˜ç‡')
        axes[0, 2].set_ylabel('ç”Ÿå­˜ç‡')
        
        # 4. å¹´é¾„åˆ†å¸ƒ
        self.train_data['Age'].hist(bins=30, ax=axes[1, 0], alpha=0.7)
        axes[1, 0].set_title('å¹´é¾„åˆ†å¸ƒ')
        axes[1, 0].set_xlabel('å¹´é¾„')
        axes[1, 0].set_ylabel('é¢‘æ•°')
        
        # 5. ç¥¨ä»·åˆ†å¸ƒ
        self.train_data['Fare'].hist(bins=30, ax=axes[1, 1], alpha=0.7)
        axes[1, 1].set_title('ç¥¨ä»·åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('ç¥¨ä»·')
        axes[1, 1].set_ylabel('é¢‘æ•°')
        
        # 6. ç™»èˆ¹æ¸¯å£ä¸ç”Ÿå­˜ç‡
        sns.barplot(data=self.train_data, x='Embarked', y='Survived', ax=axes[1, 2])
        axes[1, 2].set_title('ç™»èˆ¹æ¸¯å£ä¸ç”Ÿå­˜ç‡')
        axes[1, 2].set_ylabel('ç”Ÿå­˜ç‡')
        
        plt.tight_layout()
        plt.savefig('titanic_eda_optimized.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def advanced_feature_engineering(self):
        """
        é«˜çº§ç‰¹å¾å·¥ç¨‹ - åŸºäºChris Deotteæ–¹æ¡ˆçš„ä¼˜åŒ–
        """
        print("\n=== é«˜çº§ç‰¹å¾å·¥ç¨‹é˜¶æ®µ ===")
        
        # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®è¿›è¡Œç»Ÿä¸€å¤„ç†
        all_data = pd.concat([self.train_data, self.test_data], ignore_index=True)
        
        # 1. åŸºç¡€ç¼ºå¤±å€¼å¤„ç†
        print("å¤„ç†ç¼ºå¤±å€¼...")
        
        # Age: ä½¿ç”¨æ›´ç²¾ç»†çš„åˆ†ç»„å¡«å……
        all_data['Age'] = all_data.groupby(['Sex', 'Pclass', 'SibSp', 'Parch'])['Age'].transform(
            lambda x: x.fillna(x.median()) if not x.isna().all() else x.fillna(all_data['Age'].median())
        )
        
        # Embarked: åŸºäºç¥¨å·ä¿¡æ¯å¡«å……
        all_data.loc[all_data['Ticket'] == '113572', 'Embarked'] = 'S'
        all_data['Embarked'].fillna(all_data['Embarked'].mode()[0], inplace=True)
        
        # Fare: ä½¿ç”¨æ›´ç²¾ç»†çš„åˆ†ç»„å¡«å……
        all_data['Fare'] = all_data.groupby(['Pclass', 'Embarked'])['Fare'].transform(
            lambda x: x.fillna(x.median())
        )
        
        # 2. é«˜çº§ç‰¹å¾å·¥ç¨‹
        print("åˆ›å»ºé«˜çº§ç‰¹å¾...")
        
        # 2.1 å§“åç‰¹å¾å·¥ç¨‹
        self._extract_name_features(all_data)
        
        # 2.2 ç¥¨å·ç‰¹å¾å·¥ç¨‹
        self._extract_ticket_features(all_data)
        
        # 2.3 èˆ¹èˆ±ç‰¹å¾å·¥ç¨‹
        self._extract_cabin_features(all_data)
        
        # 2.4 å®¶åº­å’Œç¾¤ä½“ç‰¹å¾å·¥ç¨‹
        self._extract_family_group_features(all_data)
        
        # 2.5 ç¾¤ä½“ç”Ÿå­˜æ¨¡å¼ç‰¹å¾ï¼ˆä»…å¯¹è®­ç»ƒæ•°æ®ï¼‰
        self._extract_group_survival_patterns(all_data)
        
        # 2.6 å…¶ä»–é«˜çº§ç‰¹å¾
        self._extract_additional_features(all_data)
        
        # 3. ç‰¹å¾ç¼–ç å’Œæ ‡å‡†åŒ–
        self._encode_features(all_data)
        
        # 4. é€‰æ‹©æœ€ç»ˆç‰¹å¾
        self._select_final_features(self.all_data_processed)
        
        print(f"é«˜çº§ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾æ•°é‡: {len(self.feature_columns)}")
        print(f"ç‰¹å¾åˆ—è¡¨: {self.feature_columns}")
        
    def _extract_name_features(self, data):
        """
        ä»å§“åä¸­æå–ç‰¹å¾
        """
        # æå–ç§°è°“
        data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
        
        # æå–å§“æ°
        data['LastName'] = data['Name'].str.extract('([^,]+),', expand=False)
        
        # é«˜çº§ç§°è°“åˆ†ç±»ï¼ˆåŸºäºChris Deotteæ–¹æ¡ˆï¼‰
        title_mapping = {
            # ç”·æ€§ç§°è°“
            'Mr': 'Man', 'Sir': 'Man', 'Don': 'Man', 'Rev': 'Man', 
            'Major': 'Man', 'Col': 'Man', 'Capt': 'Man', 'Jonkheer': 'Man',
            'Dr': 'Man',  # å¤§éƒ¨åˆ†Dræ˜¯ç”·æ€§
            
            # å¥³æ€§ç§°è°“
            'Mrs': 'Woman', 'Miss': 'Woman', 'Mme': 'Woman', 'Ms': 'Woman',
            'Lady': 'Woman', 'Mlle': 'Woman', 'the Countess': 'Woman', 'Dona': 'Woman',
            
            # ç”·å­©ç§°è°“
            'Master': 'Boy'
        }
        
        data['Title_Group'] = data['Title'].map(title_mapping)
        data['Title_Group'].fillna('Rare', inplace=True)
        
        # å§“åé•¿åº¦ç‰¹å¾
        data['Name_Length'] = data['Name'].str.len()
        
    def _extract_ticket_features(self, data):
        """
        ä»ç¥¨å·ä¸­æå–ç‰¹å¾
        """
        # æ¸…ç†ç¥¨å·
        data['Ticket_Clean'] = data['Ticket'].str.replace('[^A-Za-z0-9]', '', regex=True)
        
        # ç¥¨å·å‰ç¼€
        data['Ticket_Prefix'] = data['Ticket'].str.extract('([A-Za-z]+)', expand=False)
        data['Ticket_Prefix'].fillna('None', inplace=True)
        
        # ç¥¨å·æ•°å­—éƒ¨åˆ†
        data['Ticket_Number'] = data['Ticket'].str.extract('(\d+)', expand=False)
        data['Ticket_Number'] = pd.to_numeric(data['Ticket_Number'], errors='coerce')
        
        # ç¥¨å·é¢‘ç‡ï¼ˆåŒä¸€ç¥¨å·çš„äººæ•°ï¼‰
        data['Ticket_Freq'] = data.groupby('Ticket')['Ticket'].transform('count')
        
        # è°ƒæ•´åçš„ç¥¨ä»·ï¼ˆäººå‡ç¥¨ä»·ï¼‰
        data['Fare_Per_Person'] = data['Fare'] / data['Ticket_Freq']
        
        # ç¾¤ä½“IDï¼ˆåŸºäºç¥¨å·å‰4ä½å’Œäººå‡ç¥¨ä»·ï¼‰
        data['Group_ID'] = (data['Ticket_Clean'].str[:4] + '_' + 
                           data['Fare_Per_Person'].round(2).astype(str))
        
    def _extract_cabin_features(self, data):
        """
        ä»èˆ¹èˆ±ä¿¡æ¯ä¸­æå–ç‰¹å¾
        """
        # æ˜¯å¦æœ‰èˆ¹èˆ±ä¿¡æ¯
        data['Has_Cabin'] = data['Cabin'].notna().astype(int)
        
        # èˆ¹èˆ±ç±»å‹ï¼ˆé¦–å­—æ¯ï¼‰
        data['Cabin_Type'] = data['Cabin'].str[0]
        data['Cabin_Type'].fillna('Missing', inplace=True)
        
        # èˆ¹èˆ±æ•°é‡
        data['Cabin_Count'] = data['Cabin'].str.count(' ') + 1
        data['Cabin_Count'] = data['Cabin_Count'].fillna(0)
        
    def _extract_family_group_features(self, data):
        """
        æå–å®¶åº­å’Œç¾¤ä½“ç‰¹å¾
        """
        # åŸºç¡€å®¶åº­ç‰¹å¾
        data['Family_Size'] = data['SibSp'] + data['Parch'] + 1
        data['Is_Alone'] = (data['Family_Size'] == 1).astype(int)
        
        # å®¶åº­ç±»å‹
        data['Family_Type'] = 'Small'
        data.loc[data['Family_Size'].between(2, 4), 'Family_Type'] = 'Medium'
        data.loc[data['Family_Size'] >= 5, 'Family_Type'] = 'Large'
        
        # ç¾¤ä½“å¤§å°ï¼ˆåŸºäºGroup_IDï¼‰
        data['Group_Size'] = data.groupby('Group_ID')['Group_ID'].transform('count')
        
        # æ˜¯å¦ä¸ºç¾¤ä½“æ—…è¡Œ
        data['Is_Group_Travel'] = (data['Group_Size'] > 1).astype(int)
        
    def _extract_group_survival_patterns(self, data):
        """
        æå–ç¾¤ä½“ç”Ÿå­˜æ¨¡å¼ç‰¹å¾ï¼ˆä»…åŸºäºè®­ç»ƒæ•°æ®ï¼‰
        """
        train_len = len(self.train_data)
        train_part = data[:train_len].copy()
        
        # è®¡ç®—å„ç¾¤ä½“çš„ç”Ÿå­˜ç‡
        group_survival = train_part.groupby('Group_ID')['Survived'].agg(['mean', 'count']).reset_index()
        group_survival.columns = ['Group_ID', 'Group_Survival_Rate', 'Group_Count']
        
        # åªè€ƒè™‘ç¾¤ä½“å¤§å°>=2çš„æƒ…å†µ
        group_survival = group_survival[group_survival['Group_Count'] >= 2]
        
        # ç¾¤ä½“ç”Ÿå­˜æ¨¡å¼
        group_survival['Group_Pattern'] = 'Mixed'
        group_survival.loc[group_survival['Group_Survival_Rate'] == 1.0, 'Group_Pattern'] = 'All_Survived'
        group_survival.loc[group_survival['Group_Survival_Rate'] == 0.0, 'Group_Pattern'] = 'All_Died'
        
        # åˆå¹¶å›åŸæ•°æ®
        data = data.merge(group_survival[['Group_ID', 'Group_Survival_Rate', 'Group_Pattern']], 
                         on='Group_ID', how='left')
        
        # å¡«å……ç¼ºå¤±å€¼
        data['Group_Survival_Rate'].fillna(0.5, inplace=True)  # ä¸­æ€§å€¼
        data['Group_Pattern'].fillna('Unknown', inplace=True)
        
        # æŒ‰ç§°è°“åˆ†ç»„çš„ç¾¤ä½“ç”Ÿå­˜æ¨¡å¼
        for title in ['Man', 'Woman', 'Boy']:
            title_group_survival = train_part[train_part['Title_Group'] == title].groupby('Group_ID')['Survived'].mean()
            data[f'{title}_Group_Survival'] = data['Group_ID'].map(title_group_survival)
            data[f'{title}_Group_Survival'].fillna(0.5, inplace=True)
        
        # æ›´æ–°self.all_data_processed
        self.all_data_processed = data
        
    def _extract_additional_features(self, data):
        """
        æå–å…¶ä»–é«˜çº§ç‰¹å¾
        """
        # å¹´é¾„åˆ†ç»„ï¼ˆæ›´ç»†è‡´ï¼‰
        data['Age_Group'] = 'Adult'
        data.loc[data['Age'] <= 12, 'Age_Group'] = 'Child'
        data.loc[(data['Age'] > 12) & (data['Age'] <= 18), 'Age_Group'] = 'Teen'
        data.loc[(data['Age'] > 18) & (data['Age'] <= 35), 'Age_Group'] = 'Young_Adult'
        data.loc[(data['Age'] > 35) & (data['Age'] <= 60), 'Age_Group'] = 'Middle_Age'
        data.loc[data['Age'] > 60, 'Age_Group'] = 'Senior'
        
        # ç¥¨ä»·åˆ†ç»„ï¼ˆåŸºäºåˆ†ä½æ•°ï¼‰
        data['Fare_Group'] = pd.qcut(data['Fare'], q=5, labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High'])
        
        # ç¤¾ä¼šåœ°ä½ç‰¹å¾
        data['Social_Status'] = 'Low'
        data.loc[(data['Pclass'] == 1) | (data['Title_Group'] == 'Woman'), 'Social_Status'] = 'High'
        data.loc[(data['Pclass'] == 2) | (data['Title_Group'] == 'Boy'), 'Social_Status'] = 'Medium'
        
        # å¹´é¾„ç¼ºå¤±æ ‡è®°
        data['Age_Missing'] = data['Age'].isna().astype(int)
        
        # æ›´æ–°å¤„ç†åçš„æ•°æ®
        self.all_data_processed = data
        
    def _encode_features(self, data):
        """
        ç¼–ç åˆ†ç±»ç‰¹å¾
        """
        # ä½¿ç”¨å·²å¤„ç†çš„æ•°æ®
        if hasattr(self, 'all_data_processed'):
            data = self.all_data_processed.copy()
        
        # æ€§åˆ«ç¼–ç 
        data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})
        
        # ç™»èˆ¹æ¸¯å£ç¼–ç 
        embarked_mapping = {'S': 0, 'C': 1, 'Q': 2}
        data['Embarked'] = data['Embarked'].map(embarked_mapping)
        
        # ç§°è°“ç»„ç¼–ç 
        title_group_mapping = {'Man': 0, 'Woman': 1, 'Boy': 2, 'Rare': 3}
        data['Title_Group'] = data['Title_Group'].map(title_group_mapping)
        
        # å¹´é¾„ç»„ç¼–ç 
        age_group_mapping = {'Child': 0, 'Teen': 1, 'Young_Adult': 2, 'Adult': 3, 'Senior': 4}
        if 'Age_Group' in data.columns:
            data['Age_Group'] = data['Age_Group'].map(age_group_mapping)
            data['Age_Group'].fillna(3, inplace=True)  # é»˜è®¤ä¸ºAdult
        else:
            data['Age_Group'] = 3
        
        # ç¥¨ä»·ç»„ç¼–ç 
        fare_group_mapping = {'Very_Low': 0, 'Low': 1, 'Medium': 2, 'High': 3, 'Very_High': 4}
        if 'Fare_Group' in data.columns:
            data['Fare_Group'] = data['Fare_Group'].map(fare_group_mapping)
            data['Fare_Group'].fillna(2, inplace=True)  # é»˜è®¤ä¸ºMedium
        else:
            data['Fare_Group'] = 2
        
        # å®¶åº­ç±»å‹ç¼–ç 
        family_type_mapping = {'Small': 0, 'Medium': 1, 'Large': 2}
        if 'Family_Type' in data.columns:
            data['Family_Type'] = data['Family_Type'].map(family_type_mapping)
        else:
            data['Family_Type'] = 0
        
        # èˆ¹èˆ±ç±»å‹ç¼–ç 
        cabin_type_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'T': 7, 'Missing': 8}
        if 'Cabin_Type' in data.columns:
            data['Cabin_Type'] = data['Cabin_Type'].map(cabin_type_mapping)
        else:
            data['Cabin_Type'] = 8
        
        # ç¾¤ä½“æ¨¡å¼ç¼–ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'Group_Pattern' in data.columns:
            pattern_mapping = {'All_Survived': 2, 'All_Died': 0, 'Mixed': 1, 'Unknown': 1}
            data['Group_Pattern'] = data['Group_Pattern'].map(pattern_mapping)
            data['Group_Pattern'].fillna(1, inplace=True)
        else:
            data['Group_Pattern'] = 1  # é»˜è®¤å€¼
        
        # ç¤¾ä¼šåœ°ä½ç¼–ç 
        status_mapping = {'Low': 0, 'Medium': 1, 'High': 2}
        if 'Social_Status' in data.columns:
            data['Social_Status'] = data['Social_Status'].map(status_mapping)
        else:
            data['Social_Status'] = 0
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        self.all_data_processed = data
        
    def _select_final_features(self, data):
        """
        é€‰æ‹©æœ€ç»ˆçš„ç‰¹å¾åˆ—
        """
        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾
        potential_features = [
            # åŸºç¡€ç‰¹å¾
            'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked',
            
            # èˆ¹èˆ±ç‰¹å¾
            'Has_Cabin', 'Cabin_Type', 'Cabin_Count',
            
            # å®¶åº­ç‰¹å¾
            'Family_Size', 'Is_Alone', 'Family_Type',
            
            # ç§°è°“ç‰¹å¾
            'Title_Group', 'Name_Length',
            
            # ç¥¨å·ç‰¹å¾
            'Ticket_Freq', 'Fare_Per_Person', 'Group_Size', 'Is_Group_Travel',
            
            # ç¾¤ä½“ç”Ÿå­˜æ¨¡å¼ç‰¹å¾
            'Group_Survival_Rate', 'Group_Pattern',
            'Man_Group_Survival', 'Woman_Group_Survival', 'Boy_Group_Survival',
            
            # åˆ†ç»„ç‰¹å¾
            'Age_Group', 'Fare_Group', 'Social_Status',
            
            # å…¶ä»–ç‰¹å¾
            'Age_Missing'
        ]
        
        # åªé€‰æ‹©å®é™…å­˜åœ¨çš„ç‰¹å¾
        self.feature_columns = [col for col in potential_features if col in data.columns]
        
        print(f"å®é™…å¯ç”¨ç‰¹å¾: {len(self.feature_columns)}ä¸ª")
        print(f"ç¼ºå¤±ç‰¹å¾: {set(potential_features) - set(self.feature_columns)}")
        
        # åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        train_len = len(self.train_data)
        self.train_processed = data[:train_len][self.feature_columns + ['Survived']]
        self.test_processed = data[train_len:][self.feature_columns]
        
    def train_optimized_models(self):
        """
        è®­ç»ƒä¼˜åŒ–çš„æœºå™¨å­¦ä¹ æ¨¡å‹
        """
        print("\n=== ä¼˜åŒ–æ¨¡å‹è®­ç»ƒé˜¶æ®µ ===")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = self.train_processed.drop('Survived', axis=1)
        y = self.train_processed['Survived']
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # å®šä¹‰ä¼˜åŒ–çš„æ¨¡å‹
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=200, max_depth=8, min_samples_split=4,
                min_samples_leaf=2, random_state=42, n_jobs=-1
            ),
            'XGBoost': xgb.XGBClassifier(
                n_estimators=200, max_depth=6, learning_rate=0.1,
                subsample=0.8, colsample_bytree=0.8, random_state=42
            ),
            'GradientBoosting': GradientBoostingClassifier(
                n_estimators=200, learning_rate=0.1, max_depth=6, 
                subsample=0.8, random_state=42
            ),
            'LogisticRegression': LogisticRegression(
                C=0.1, random_state=42, max_iter=1000
            ),
            'SVM': SVC(
                C=10, kernel='rbf', gamma='scale', probability=True, random_state=42
            )
        }
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
        model_scores = {}
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for name, model in models.items():
            print(f"\nè®­ç»ƒ {name} æ¨¡å‹...")
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
            model_scores[name] = cv_scores.mean()
            
            print(f"{name} äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train, y_train)
            
            # éªŒè¯é›†é¢„æµ‹
            val_pred = model.predict(X_val)
            val_accuracy = accuracy_score(y_val, val_pred)
            print(f"{name} éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f}")
            
            # ä¿å­˜æ¨¡å‹
            self.models[name] = model
        
        # åˆ›å»ºä¼˜åŒ–çš„é›†æˆæ¨¡å‹
        print("\nåˆ›å»ºä¼˜åŒ–é›†æˆæ¨¡å‹...")
        voting_clf = VotingClassifier(
            estimators=[
                ('rf', models['RandomForest']),
                ('xgb', models['XGBoost']),
                ('gb', models['GradientBoosting']),
                ('svm', models['SVM'])
            ],
            voting='soft'
        )
        
        # è®­ç»ƒé›†æˆæ¨¡å‹
        voting_clf.fit(X_train, y_train)
        
        # è¯„ä¼°é›†æˆæ¨¡å‹
        ensemble_cv_scores = cross_val_score(voting_clf, X_train, y_train, cv=cv, scoring='accuracy')
        ensemble_val_pred = voting_clf.predict(X_val)
        ensemble_val_accuracy = accuracy_score(y_val, ensemble_val_pred)
        
        print(f"ä¼˜åŒ–é›†æˆæ¨¡å‹äº¤å‰éªŒè¯å‡†ç¡®ç‡: {ensemble_cv_scores.mean():.4f} (+/- {ensemble_cv_scores.std() * 2:.4f})")
        print(f"ä¼˜åŒ–é›†æˆæ¨¡å‹éªŒè¯é›†å‡†ç¡®ç‡: {ensemble_val_accuracy:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        self.models['Optimized_Ensemble'] = voting_clf
        model_scores['Optimized_Ensemble'] = ensemble_cv_scores.mean()
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model_name = max(model_scores, key=model_scores.get)
        self.best_model = self.models[best_model_name]
        
        print(f"\næœ€ä½³æ¨¡å‹: {best_model_name} (å‡†ç¡®ç‡: {model_scores[best_model_name]:.4f})")
        
        return model_scores
    
    def hyperparameter_optimization(self):
        """
        è¶…å‚æ•°ä¼˜åŒ–
        """
        print("\n=== è¶…å‚æ•°ä¼˜åŒ–é˜¶æ®µ ===")
        
        # å‡†å¤‡æ•°æ®
        X = self.train_processed.drop('Survived', axis=1)
        y = self.train_processed['Survived']
        X_scaled = self.scaler.fit_transform(X)
        
        # XGBoostè¶…å‚æ•°ä¼˜åŒ–
        print("ä¼˜åŒ–XGBoostè¶…å‚æ•°...")
        xgb_param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [4, 6, 8],
            'learning_rate': [0.05, 0.1, 0.15],
            'subsample': [0.8, 0.9],
            'colsample_bytree': [0.8, 0.9]
        }
        
        xgb_grid = GridSearchCV(
            xgb.XGBClassifier(random_state=42),
            xgb_param_grid,
            cv=3,
            scoring='accuracy',
            n_jobs=-1,
            verbose=1
        )
        
        xgb_grid.fit(X_scaled, y)
        
        print(f"XGBoostæœ€ä½³å‚æ•°: {xgb_grid.best_params_}")
        print(f"XGBoostæœ€ä½³å¾—åˆ†: {xgb_grid.best_score_:.4f}")
        
        # ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
        self.models['XGBoost_Optimized'] = xgb_grid.best_estimator_
        
        return xgb_grid.best_estimator_
    
    def make_predictions(self):
        """
        ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹
        """
        print("\n=== é¢„æµ‹é˜¶æ®µ ===")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        X_test = self.test_processed
        X_test_scaled = self.scaler.transform(X_test)
        
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹
        predictions = self.best_model.predict(X_test_scaled)
        
        # åˆ›å»ºæäº¤æ–‡ä»¶
        submission = pd.DataFrame({
            'PassengerId': self.test_data['PassengerId'],
            'Survived': predictions.astype(int)
        })
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        submission.to_csv('titanic_submission_optimized.csv', index=False)
        print("é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° titanic_submission_optimized.csv")
        
        # æ˜¾ç¤ºé¢„æµ‹ç»Ÿè®¡
        survival_rate = predictions.mean()
        print(f"é¢„æµ‹ç”Ÿå­˜ç‡: {survival_rate:.2%}")
        print(f"é¢„æµ‹ç”Ÿå­˜äººæ•°: {predictions.sum()}/{len(predictions)}")
        
        return submission
    
    def feature_importance_analysis(self):
        """
        åˆ†æç‰¹å¾é‡è¦æ€§
        """
        print("\n=== ç‰¹å¾é‡è¦æ€§åˆ†æ ===")
        
        # è·å–XGBoostæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        if 'XGBoost' in self.models:
            xgb_model = self.models['XGBoost']
            feature_names = self.feature_columns
            
            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
            importance_df = pd.DataFrame({
                'ç‰¹å¾': feature_names,
                'é‡è¦æ€§': xgb_model.feature_importances_
            }).sort_values('é‡è¦æ€§', ascending=False)
            
            print("\nç‰¹å¾é‡è¦æ€§æ’åº (XGBoost):")
            for idx, row in importance_df.head(15).iterrows():
                print(f"{row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}")
            
            # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
            plt.figure(figsize=(12, 10))
            sns.barplot(data=importance_df.head(15), x='é‡è¦æ€§', y='ç‰¹å¾')
            plt.title('Top 15 ç‰¹å¾é‡è¦æ€§ (ä¼˜åŒ–ç‰ˆæœ¬)')
            plt.xlabel('é‡è¦æ€§åˆ†æ•°')
            plt.tight_layout()
            plt.savefig('feature_importance_optimized.png', dpi=300, bbox_inches='tight')
            plt.show()
    
    def run_complete_optimized_pipeline(self, train_path, test_path):
        """
        è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–æœºå™¨å­¦ä¹ æµæ°´çº¿
        """
        print("ğŸš¢ æ³°å¦å°¼å…‹å·ç”Ÿå­˜é¢„æµ‹ - ä¼˜åŒ–ç‰ˆæœ¬æµæ°´çº¿å¯åŠ¨ ğŸš¢")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data(train_path, test_path)
        
        # 2. æ¢ç´¢æ€§æ•°æ®åˆ†æ
        self.exploratory_data_analysis()
        
        # 3. é«˜çº§ç‰¹å¾å·¥ç¨‹
        self.advanced_feature_engineering()
        
        # 4. æ¨¡å‹è®­ç»ƒ
        model_scores = self.train_optimized_models()
        
        # 5. è¶…å‚æ•°ä¼˜åŒ–
        try:
            optimized_model = self.hyperparameter_optimization()
            # æ›´æ–°æœ€ä½³æ¨¡å‹
            if 'XGBoost_Optimized' in self.models:
                cv_scores = cross_val_score(optimized_model, 
                                          self.scaler.transform(self.train_processed.drop('Survived', axis=1)), 
                                          self.train_processed['Survived'], 
                                          cv=5, scoring='accuracy')
                model_scores['XGBoost_Optimized'] = cv_scores.mean()
                if cv_scores.mean() > max([score for name, score in model_scores.items() if name != 'XGBoost_Optimized']):
                    self.best_model = optimized_model
        except Exception as e:
            print(f"è¶…å‚æ•°ä¼˜åŒ–è·³è¿‡: {e}")
        
        # 6. ç‰¹å¾é‡è¦æ€§åˆ†æ
        self.feature_importance_analysis()
        
        # 7. é¢„æµ‹
        submission = self.make_predictions()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ä¼˜åŒ–ç‰ˆæœ¬æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½æ€»ç»“:")
        for model_name, score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True):
            print(f"   {model_name}: {score:.4f}")
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print("   - titanic_submission_optimized.csv (ä¼˜åŒ–é¢„æµ‹ç»“æœ)")
        print("   - titanic_eda_optimized.png (æ•°æ®åˆ†æå›¾è¡¨)")
        print("   - feature_importance_optimized.png (ç‰¹å¾é‡è¦æ€§å›¾è¡¨)")
        
        return submission

# ä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºä¼˜åŒ–é¢„æµ‹å™¨å®ä¾‹
    predictor = TitanicSurvivalPredictorOptimized()
    
    # è¿è¡Œå®Œæ•´ä¼˜åŒ–æµæ°´çº¿
    submission = predictor.run_complete_optimized_pipeline('train.csv', 'test.csv')
    
    print("\nğŸ† ä¼˜åŒ–ç‰ˆæœ¬é¢„æµ‹å®Œæˆï¼æœŸå¾…æ›´é«˜çš„Kaggleåˆ†æ•°ï¼")